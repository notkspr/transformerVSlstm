[
  {
    "epoch": 1,
    "phase": "pretrain",
    "model_name": "gpt",
    "timestamp": 1763730039.1655858,
    "metrics": {
      "train_loss": 7.361358354731304,
      "perplexity": 1573.9731253661585,
      "total_tokens": 5337660.0,
      "epoch_time": 961.7183647155762,
      "elapsed_time": 961.7185099124908
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6895746,
    "model_display_name": "Transformer"
  },
  {
    "epoch": 2,
    "phase": "pretrain",
    "model_name": "gpt",
    "timestamp": 1763731001.2484987,
    "metrics": {
      "train_loss": 6.141507044071105,
      "perplexity": 464.75344728175907,
      "total_tokens": 5337660.0,
      "epoch_time": 962.043258190155,
      "elapsed_time": 1923.8013925552368
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6895746,
    "model_display_name": "Transformer"
  },
  {
    "epoch": 3,
    "phase": "pretrain",
    "model_name": "gpt",
    "timestamp": 1763731963.0465426,
    "metrics": {
      "train_loss": 5.687978154275475,
      "perplexity": 295.295973767157,
      "total_tokens": 5337660.0,
      "epoch_time": 961.7478392124176,
      "elapsed_time": 2885.5994639396667
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6895746,
    "model_display_name": "Transformer"
  },
  {
    "epoch": 4,
    "phase": "pretrain",
    "model_name": "gpt",
    "timestamp": 1763732930.6740296,
    "metrics": {
      "train_loss": 5.355909718245995,
      "perplexity": 211.85661854597544,
      "total_tokens": 5337660.0,
      "epoch_time": 967.5841879844666,
      "elapsed_time": 3853.2269151210785
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6895746,
    "model_display_name": "Transformer"
  },
  {
    "epoch": 5,
    "phase": "pretrain",
    "model_name": "gpt",
    "timestamp": 1763733896.351149,
    "metrics": {
      "train_loss": 5.080823497074406,
      "perplexity": 160.90650742252865,
      "total_tokens": 5337660.0,
      "epoch_time": 965.6445472240448,
      "elapsed_time": 4818.904032707214
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6895746,
    "model_display_name": "Transformer"
  },
  {
    "epoch": 6,
    "phase": "pretrain",
    "model_name": "gpt",
    "timestamp": 1763734861.7493048,
    "metrics": {
      "train_loss": 4.469642276080643,
      "perplexity": 87.32547901279884,
      "total_tokens": 5337660.0,
      "epoch_time": 965.3205053806305,
      "elapsed_time": 5784.302051782608
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6895746,
    "model_display_name": "Transformer"
  },
  {
    "epoch": 7,
    "phase": "pretrain",
    "model_name": "gpt",
    "timestamp": 1763735830.619645,
    "metrics": {
      "train_loss": 0.7598318551190015,
      "perplexity": 2.1379167105219095,
      "total_tokens": 5337660.0,
      "epoch_time": 968.83509516716,
      "elapsed_time": 6753.172370433807
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6895746,
    "model_display_name": "Transformer"
  },
  {
    "epoch": 8,
    "phase": "pretrain",
    "model_name": "gpt",
    "timestamp": 1763736989.7710714,
    "metrics": {
      "train_loss": 0.17414946967690456,
      "perplexity": 1.190233456335572,
      "total_tokens": 5337660.0,
      "epoch_time": 1159.1076946258545,
      "elapsed_time": 7912.323448181152
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6895746,
    "model_display_name": "Transformer"
  }
]