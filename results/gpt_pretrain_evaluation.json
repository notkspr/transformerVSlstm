[
  {
    "epoch": 1,
    "phase": "pretrain",
    "model_name": "gpt",
    "timestamp": 1763667851.3644578,
    "metrics": {
      "train_loss": 7.3420422411546475,
      "perplexity": 1543.861834502234,
      "total_tokens": 5337660.0,
      "epoch_time": 1158.9959824085236,
      "elapsed_time": 1158.9960544109344
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6895746,
    "model_display_name": "Transformer"
  },
  {
    "epoch": 2,
    "phase": "pretrain",
    "model_name": "gpt",
    "timestamp": 1763669010.7480564,
    "metrics": {
      "train_loss": 6.1361005408007925,
      "perplexity": 462.247536460702,
      "total_tokens": 5337660.0,
      "epoch_time": 1159.326476573944,
      "elapsed_time": 2318.3796679973602
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6895746,
    "model_display_name": "Transformer"
  },
  {
    "epoch": 3,
    "phase": "pretrain",
    "model_name": "gpt",
    "timestamp": 1763670170.2315009,
    "metrics": {
      "train_loss": 5.67976873386197,
      "perplexity": 292.88168847270776,
      "total_tokens": 5337660.0,
      "epoch_time": 1159.43812084198,
      "elapsed_time": 3477.8630323410034
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6895746,
    "model_display_name": "Transformer"
  },
  {
    "epoch": 4,
    "phase": "pretrain",
    "model_name": "gpt",
    "timestamp": 1763671329.9532647,
    "metrics": {
      "train_loss": 5.333728932752842,
      "perplexity": 207.20920451899062,
      "total_tokens": 5337660.0,
      "epoch_time": 1159.663646697998,
      "elapsed_time": 4637.5843024253845
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6895746,
    "model_display_name": "Transformer"
  },
  {
    "epoch": 5,
    "phase": "pretrain",
    "model_name": "gpt",
    "timestamp": 1763672489.2872803,
    "metrics": {
      "train_loss": 5.0358942063843335,
      "perplexity": 153.8370932366316,
      "total_tokens": 5337660.0,
      "epoch_time": 1159.277931690216,
      "elapsed_time": 5796.91886973381
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6895746,
    "model_display_name": "Transformer"
  },
  {
    "epoch": 6,
    "phase": "pretrain",
    "model_name": "gpt",
    "timestamp": 1763673648.552094,
    "metrics": {
      "train_loss": 2.0492953131111658,
      "perplexity": 7.762429096496623,
      "total_tokens": 5337660.0,
      "epoch_time": 1159.2016983032227,
      "elapsed_time": 6956.183678388596
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6895746,
    "model_display_name": "Transformer"
  },
  {
    "epoch": 7,
    "phase": "pretrain",
    "model_name": "gpt",
    "timestamp": 1763674808.0210927,
    "metrics": {
      "train_loss": 0.7140486194956593,
      "perplexity": 2.0422428077306085,
      "total_tokens": 5337660.0,
      "epoch_time": 1159.4248909950256,
      "elapsed_time": 8115.652697324753
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6895746,
    "model_display_name": "Transformer"
  },
  {
    "epoch": 8,
    "phase": "pretrain",
    "model_name": "gpt",
    "timestamp": 1763675967.268493,
    "metrics": {
      "train_loss": 0.597956966154459,
      "perplexity": 1.8183999501726815,
      "total_tokens": 5337660.0,
      "epoch_time": 1159.215103149414,
      "elapsed_time": 9274.89999961853
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6895746,
    "model_display_name": "Transformer"
  }
]