[
  {
    "epoch": 1,
    "phase": "pretrain",
    "model_name": "gpt",
    "timestamp": 1763750319.411575,
    "metrics": {
      "train_loss": 7.34902347878712,
      "perplexity": 1554.6776106644331,
      "total_tokens": 5337660.0,
      "epoch_time": 1075.3281693458557,
      "elapsed_time": 1075.3282432556152
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6895746,
    "model_display_name": "Transformer"
  },
  {
    "epoch": 2,
    "phase": "pretrain",
    "model_name": "gpt",
    "timestamp": 1763751366.2287219,
    "metrics": {
      "train_loss": 6.160904759314002,
      "perplexity": 473.8566071251095,
      "total_tokens": 5337660.0,
      "epoch_time": 1046.721025466919,
      "elapsed_time": 2122.1453869342804
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6895746,
    "model_display_name": "Transformer"
  },
  {
    "epoch": 3,
    "phase": "pretrain",
    "model_name": "gpt",
    "timestamp": 1763752376.2879434,
    "metrics": {
      "train_loss": 5.709407034443646,
      "perplexity": 301.69212219651473,
      "total_tokens": 5337660.0,
      "epoch_time": 1009.977778673172,
      "elapsed_time": 3132.2044925689697
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6895746,
    "model_display_name": "Transformer"
  },
  {
    "epoch": 4,
    "phase": "pretrain",
    "model_name": "gpt",
    "timestamp": 1763753386.8665953,
    "metrics": {
      "train_loss": 5.387888213483299,
      "perplexity": 218.74096324038302,
      "total_tokens": 5337660.0,
      "epoch_time": 1010.4987487792969,
      "elapsed_time": 4142.78323507309
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6895746,
    "model_display_name": "Transformer"
  },
  {
    "epoch": 5,
    "phase": "pretrain",
    "model_name": "gpt",
    "timestamp": 1763754397.1353087,
    "metrics": {
      "train_loss": 5.11828147783512,
      "perplexity": 167.04804703527094,
      "total_tokens": 5337660.0,
      "epoch_time": 1010.1862945556641,
      "elapsed_time": 5153.052022695541
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6895746,
    "model_display_name": "Transformer"
  },
  {
    "epoch": 6,
    "phase": "pretrain",
    "model_name": "gpt",
    "timestamp": 1763755406.99331,
    "metrics": {
      "train_loss": 4.8819861106756255,
      "perplexity": 131.89235674207774,
      "total_tokens": 5337660.0,
      "epoch_time": 1009.7791304588318,
      "elapsed_time": 6162.909979104996
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6895746,
    "model_display_name": "Transformer"
  },
  {
    "epoch": 7,
    "phase": "pretrain",
    "model_name": "gpt",
    "timestamp": 1763756416.9825518,
    "metrics": {
      "train_loss": 2.471046352804434,
      "perplexity": 11.834823776250493,
      "total_tokens": 5337660.0,
      "epoch_time": 1009.9104647636414,
      "elapsed_time": 7172.899256467819
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6895746,
    "model_display_name": "Transformer"
  },
  {
    "epoch": 8,
    "phase": "pretrain",
    "model_name": "gpt",
    "timestamp": 1763757426.736062,
    "metrics": {
      "train_loss": 0.2766752297104132,
      "perplexity": 1.3187380145531191,
      "total_tokens": 5337660.0,
      "epoch_time": 1009.6781299114227,
      "elapsed_time": 8182.652771949768
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6895746,
    "model_display_name": "Transformer"
  }
]