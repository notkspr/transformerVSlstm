{
  "model_name": "gpt",
  "total_pretrain_epochs": 8,
  "total_finetune_epochs": 0,
  "pretrain_history": [
    {
      "epoch": 1,
      "phase": "pretrain",
      "model_name": "gpt",
      "timestamp": 1763693330.397393,
      "metrics": {
        "train_loss": 7.454113262455638,
        "perplexity": 1726.9519629696676,
        "total_tokens": 5337660.0,
        "epoch_time": 932.5987975597382,
        "elapsed_time": 932.5988459587097
      },
      "learning_rate": 0.0005,
      "batch_size": 32,
      "dataset_size": 10466,
      "total_parameters": 6697474,
      "model_display_name": "Transformer"
    },
    {
      "epoch": 2,
      "phase": "pretrain",
      "model_name": "gpt",
      "timestamp": 1763694262.9744184,
      "metrics": {
        "train_loss": 6.253874044592788,
        "perplexity": 520.0235217009784,
        "total_tokens": 5337660.0,
        "epoch_time": 932.5147435665131,
        "elapsed_time": 1865.1757562160492
      },
      "learning_rate": 0.0005,
      "batch_size": 32,
      "dataset_size": 10466,
      "total_parameters": 6697474,
      "model_display_name": "Transformer"
    },
    {
      "epoch": 3,
      "phase": "pretrain",
      "model_name": "gpt",
      "timestamp": 1763695195.078643,
      "metrics": {
        "train_loss": 5.775013878578093,
        "perplexity": 322.14890565619606,
        "total_tokens": 5337660.0,
        "epoch_time": 932.0134491920471,
        "elapsed_time": 2797.2799293994904
      },
      "learning_rate": 0.0005,
      "batch_size": 32,
      "dataset_size": 10466,
      "total_parameters": 6697474,
      "model_display_name": "Transformer"
    },
    {
      "epoch": 4,
      "phase": "pretrain",
      "model_name": "gpt",
      "timestamp": 1763696127.2283812,
      "metrics": {
        "train_loss": 5.421750087563584,
        "perplexity": 226.27477685909793,
        "total_tokens": 5337660.0,
        "epoch_time": 932.0829689502716,
        "elapsed_time": 3729.429677248001
      },
      "learning_rate": 0.0005,
      "batch_size": 32,
      "dataset_size": 10466,
      "total_parameters": 6697474,
      "model_display_name": "Transformer"
    },
    {
      "epoch": 5,
      "phase": "pretrain",
      "model_name": "gpt",
      "timestamp": 1763697059.8017058,
      "metrics": {
        "train_loss": 5.13207250542757,
        "perplexity": 169.36777017809973,
        "total_tokens": 5337660.0,
        "epoch_time": 932.5013470649719,
        "elapsed_time": 4662.003100633621
      },
      "learning_rate": 0.0005,
      "batch_size": 32,
      "dataset_size": 10466,
      "total_parameters": 6697474,
      "model_display_name": "Transformer"
    },
    {
      "epoch": 6,
      "phase": "pretrain",
      "model_name": "gpt",
      "timestamp": 1763697991.9629622,
      "metrics": {
        "train_loss": 3.1882570952903935,
        "perplexity": 24.24613189777444,
        "total_tokens": 5337660.0,
        "epoch_time": 932.088395357132,
        "elapsed_time": 5594.164252281189
      },
      "learning_rate": 0.0005,
      "batch_size": 32,
      "dataset_size": 10466,
      "total_parameters": 6697474,
      "model_display_name": "Transformer"
    },
    {
      "epoch": 7,
      "phase": "pretrain",
      "model_name": "gpt",
      "timestamp": 1763698923.9748976,
      "metrics": {
        "train_loss": 0.40222690999507904,
        "perplexity": 1.4951505587936271,
        "total_tokens": 5337660.0,
        "epoch_time": 931.9441528320312,
        "elapsed_time": 6526.17635846138
      },
      "learning_rate": 0.0005,
      "batch_size": 32,
      "dataset_size": 10466,
      "total_parameters": 6697474,
      "model_display_name": "Transformer"
    },
    {
      "epoch": 8,
      "phase": "pretrain",
      "model_name": "gpt",
      "timestamp": 1763699856.092929,
      "metrics": {
        "train_loss": 0.15606568833222476,
        "perplexity": 1.1689029838555833,
        "total_tokens": 5337660.0,
        "epoch_time": 932.0434193611145,
        "elapsed_time": 7458.294357299805
      },
      "learning_rate": 0.0005,
      "batch_size": 32,
      "dataset_size": 10466,
      "total_parameters": 6697474,
      "model_display_name": "Transformer"
    }
  ],
  "finetune_history": [],
  "best_pretrain": {
    "epoch": 8,
    "loss": 0.15606568833222476,
    "perplexity": 1.1689029838555833
  }
}