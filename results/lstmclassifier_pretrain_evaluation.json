[
  {
    "epoch": 1,
    "phase": "pretrain",
    "model_name": "lstmclassifier",
    "timestamp": 1763685695.5352504,
    "metrics": {
      "train_loss": 7.542451320624933,
      "perplexity": 1886.4486524714123,
      "total_tokens": 5337660.0,
      "epoch_time": 894.1724987030029,
      "elapsed_time": 894.1725568771362
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6632322,
    "model_display_name": "LSTM with Attention"
  },
  {
    "epoch": 2,
    "phase": "pretrain",
    "model_name": "lstmclassifier",
    "timestamp": 1763686587.3956025,
    "metrics": {
      "train_loss": 6.236055960015553,
      "perplexity": 510.8397601032025,
      "total_tokens": 5337660.0,
      "epoch_time": 891.8151714801788,
      "elapsed_time": 1786.032885313034
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6632322,
    "model_display_name": "LSTM with Attention"
  },
  {
    "epoch": 3,
    "phase": "pretrain",
    "model_name": "lstmclassifier",
    "timestamp": 1763687484.4667695,
    "metrics": {
      "train_loss": 5.706980117937413,
      "perplexity": 300.9608283588372,
      "total_tokens": 5337660.0,
      "epoch_time": 897.0459673404694,
      "elapsed_time": 2683.1042850017548
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6632322,
    "model_display_name": "LSTM with Attention"
  },
  {
    "epoch": 4,
    "phase": "pretrain",
    "model_name": "lstmclassifier",
    "timestamp": 1763688385.7104592,
    "metrics": {
      "train_loss": 3.3295271055000586,
      "perplexity": 27.925132938529806,
      "total_tokens": 5337660.0,
      "epoch_time": 901.2189202308655,
      "elapsed_time": 3584.347972393036
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6632322,
    "model_display_name": "LSTM with Attention"
  },
  {
    "epoch": 5,
    "phase": "pretrain",
    "model_name": "lstmclassifier",
    "timestamp": 1763689287.2151654,
    "metrics": {
      "train_loss": 0.8934667744651074,
      "perplexity": 2.443586346842544,
      "total_tokens": 5337660.0,
      "epoch_time": 901.4811086654663,
      "elapsed_time": 4485.85250377655
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6632322,
    "model_display_name": "LSTM with Attention"
  },
  {
    "epoch": 6,
    "phase": "pretrain",
    "model_name": "lstmclassifier",
    "timestamp": 1763690188.194487,
    "metrics": {
      "train_loss": 0.3480643688178644,
      "perplexity": 1.4163234138697929,
      "total_tokens": 5337660.0,
      "epoch_time": 900.9515602588654,
      "elapsed_time": 5386.831972837448
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6632322,
    "model_display_name": "LSTM with Attention"
  },
  {
    "epoch": 7,
    "phase": "pretrain",
    "model_name": "lstmclassifier",
    "timestamp": 1763691089.1208274,
    "metrics": {
      "train_loss": 0.2022040506325117,
      "perplexity": 1.2240977605517165,
      "total_tokens": 5337660.0,
      "epoch_time": 900.9020256996155,
      "elapsed_time": 6287.758356332779
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6632322,
    "model_display_name": "LSTM with Attention"
  },
  {
    "epoch": 8,
    "phase": "pretrain",
    "model_name": "lstmclassifier",
    "timestamp": 1763691990.7676284,
    "metrics": {
      "train_loss": 0.14607950243190293,
      "perplexity": 1.1572881916191402,
      "total_tokens": 5337660.0,
      "epoch_time": 901.6146788597107,
      "elapsed_time": 7189.40482711792
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6632322,
    "model_display_name": "LSTM with Attention"
  }
]