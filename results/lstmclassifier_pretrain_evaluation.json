[
  {
    "epoch": 1,
    "phase": "pretrain",
    "model_name": "lstmclassifier",
    "timestamp": 1763660493.7727106,
    "metrics": {
      "train_loss": 7.511447195599719,
      "perplexity": 1828.8583443308771,
      "total_tokens": 5337660.0,
      "epoch_time": 837.2509894371033,
      "elapsed_time": 837.2510883808136
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6830978,
    "model_display_name": "LSTM with Attention"
  },
  {
    "epoch": 2,
    "phase": "pretrain",
    "model_name": "lstmclassifier",
    "timestamp": 1763661327.4160545,
    "metrics": {
      "train_loss": 6.172211754612807,
      "perplexity": 479.24490688733323,
      "total_tokens": 5337660.0,
      "epoch_time": 833.559193611145,
      "elapsed_time": 1670.8945212364197
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6830978,
    "model_display_name": "LSTM with Attention"
  },
  {
    "epoch": 3,
    "phase": "pretrain",
    "model_name": "lstmclassifier",
    "timestamp": 1763662161.1983478,
    "metrics": {
      "train_loss": 5.621081230117054,
      "perplexity": 276.1878444684719,
      "total_tokens": 5337660.0,
      "epoch_time": 833.7095696926117,
      "elapsed_time": 2504.676813364029
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6830978,
    "model_display_name": "LSTM with Attention"
  },
  {
    "epoch": 4,
    "phase": "pretrain",
    "model_name": "lstmclassifier",
    "timestamp": 1763662995.1508114,
    "metrics": {
      "train_loss": 4.012836956759778,
      "perplexity": 55.30354198062893,
      "total_tokens": 5337660.0,
      "epoch_time": 833.8795883655548,
      "elapsed_time": 3338.6293017864227
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6830978,
    "model_display_name": "LSTM with Attention"
  },
  {
    "epoch": 5,
    "phase": "pretrain",
    "model_name": "lstmclassifier",
    "timestamp": 1763663829.5135314,
    "metrics": {
      "train_loss": 0.880412020425244,
      "perplexity": 2.411893251005745,
      "total_tokens": 5337660.0,
      "epoch_time": 834.2894942760468,
      "elapsed_time": 4172.992021083832
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6830978,
    "model_display_name": "LSTM with Attention"
  },
  {
    "epoch": 6,
    "phase": "pretrain",
    "model_name": "lstmclassifier",
    "timestamp": 1763664663.6700432,
    "metrics": {
      "train_loss": 0.3101750230189504,
      "perplexity": 1.363663765795924,
      "total_tokens": 5337660.0,
      "epoch_time": 834.0882015228271,
      "elapsed_time": 5007.148531198502
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6830978,
    "model_display_name": "LSTM with Attention"
  },
  {
    "epoch": 7,
    "phase": "pretrain",
    "model_name": "lstmclassifier",
    "timestamp": 1763665497.983106,
    "metrics": {
      "train_loss": 0.17473588506804733,
      "perplexity": 1.1909316322439618,
      "total_tokens": 5337660.0,
      "epoch_time": 834.2437448501587,
      "elapsed_time": 5841.461576223373
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6830978,
    "model_display_name": "LSTM with Attention"
  },
  {
    "epoch": 8,
    "phase": "pretrain",
    "model_name": "lstmclassifier",
    "timestamp": 1763666332.4804919,
    "metrics": {
      "train_loss": 0.12711632324428093,
      "perplexity": 1.1355491008546235,
      "total_tokens": 5337660.0,
      "epoch_time": 834.4327321052551,
      "elapsed_time": 6675.958944320679
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6830978,
    "model_display_name": "LSTM with Attention"
  }
]