[
  {
    "epoch": 1,
    "phase": "pretrain",
    "model_name": "lstmclassifier",
    "timestamp": 1763741310.0429597,
    "metrics": {
      "train_loss": 7.525088090722154,
      "perplexity": 1853.9765370301056,
      "total_tokens": 5337660.0,
      "epoch_time": 977.1559448242188,
      "elapsed_time": 977.1560161113739
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6830978,
    "model_display_name": "LSTM with Attention"
  },
  {
    "epoch": 2,
    "phase": "pretrain",
    "model_name": "lstmclassifier",
    "timestamp": 1763742331.756738,
    "metrics": {
      "train_loss": 6.17587365173712,
      "perplexity": 481.0030695744782,
      "total_tokens": 5337660.0,
      "epoch_time": 1021.6688189506531,
      "elapsed_time": 1998.869877576828
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6830978,
    "model_display_name": "LSTM with Attention"
  },
  {
    "epoch": 3,
    "phase": "pretrain",
    "model_name": "lstmclassifier",
    "timestamp": 1763743353.2969134,
    "metrics": {
      "train_loss": 5.626073582870204,
      "perplexity": 277.57011914464783,
      "total_tokens": 5337660.0,
      "epoch_time": 1021.4648330211639,
      "elapsed_time": 3020.409961938858
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6830978,
    "model_display_name": "LSTM with Attention"
  },
  {
    "epoch": 4,
    "phase": "pretrain",
    "model_name": "lstmclassifier",
    "timestamp": 1763744375.2250075,
    "metrics": {
      "train_loss": 5.271426491621064,
      "perplexity": 194.6934930873281,
      "total_tokens": 5337660.0,
      "epoch_time": 1021.8409116268158,
      "elapsed_time": 4042.33819770813
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6830978,
    "model_display_name": "LSTM with Attention"
  },
  {
    "epoch": 5,
    "phase": "pretrain",
    "model_name": "lstmclassifier",
    "timestamp": 1763745397.7118163,
    "metrics": {
      "train_loss": 3.3532230337945426,
      "perplexity": 28.594747119155773,
      "total_tokens": 5337660.0,
      "epoch_time": 1022.4112279415131,
      "elapsed_time": 5064.824999570847
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6830978,
    "model_display_name": "LSTM with Attention"
  },
  {
    "epoch": 6,
    "phase": "pretrain",
    "model_name": "lstmclassifier",
    "timestamp": 1763746419.1521952,
    "metrics": {
      "train_loss": 1.04953105656839,
      "perplexity": 2.8563113555024935,
      "total_tokens": 5337660.0,
      "epoch_time": 1021.3631129264832,
      "elapsed_time": 6086.2653884887695
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6830978,
    "model_display_name": "LSTM with Attention"
  },
  {
    "epoch": 7,
    "phase": "pretrain",
    "model_name": "lstmclassifier",
    "timestamp": 1763747441.1934178,
    "metrics": {
      "train_loss": 0.3880108091525915,
      "perplexity": 1.474045717387116,
      "total_tokens": 5337660.0,
      "epoch_time": 1021.9587247371674,
      "elapsed_time": 7108.306564331055
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6830978,
    "model_display_name": "LSTM with Attention"
  },
  {
    "epoch": 8,
    "phase": "pretrain",
    "model_name": "lstmclassifier",
    "timestamp": 1763748462.7631936,
    "metrics": {
      "train_loss": 0.21302579220656942,
      "perplexity": 1.2374165665357126,
      "total_tokens": 5337660.0,
      "epoch_time": 1021.4873566627502,
      "elapsed_time": 8129.876345872879
    },
    "learning_rate": 0.0005,
    "batch_size": 32,
    "dataset_size": 10466,
    "total_parameters": 6830978,
    "model_display_name": "LSTM with Attention"
  }
]