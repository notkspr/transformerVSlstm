{
  "model_name": "lstmclassifier",
  "total_pretrain_epochs": 8,
  "total_finetune_epochs": 0,
  "pretrain_history": [
    {
      "epoch": 1,
      "phase": "pretrain",
      "model_name": "lstmclassifier",
      "timestamp": 1763640279.6840568,
      "metrics": {
        "train_loss": 7.547749560053756,
        "perplexity": 1896.4700334976396,
        "total_tokens": 5337660.0,
        "epoch_time": 1148.2756078243256,
        "elapsed_time": 1148.2756683826447
      },
      "learning_rate": 0.0005,
      "batch_size": 32,
      "dataset_size": 10466,
      "total_parameters": 6632322,
      "model_display_name": "LSTM with Attention"
    },
    {
      "epoch": 2,
      "phase": "pretrain",
      "model_name": "lstmclassifier",
      "timestamp": 1763641425.6536703,
      "metrics": {
        "train_loss": 6.263463116273647,
        "perplexity": 525.0340492871613,
        "total_tokens": 5337660.0,
        "epoch_time": 1145.9334490299225,
        "elapsed_time": 2294.245470762253
      },
      "learning_rate": 0.0005,
      "batch_size": 32,
      "dataset_size": 10466,
      "total_parameters": 6632322,
      "model_display_name": "LSTM with Attention"
    },
    {
      "epoch": 3,
      "phase": "pretrain",
      "model_name": "lstmclassifier",
      "timestamp": 1763642571.3124688,
      "metrics": {
        "train_loss": 5.762293721117625,
        "perplexity": 318.0770729297941,
        "total_tokens": 5337660.0,
        "epoch_time": 1145.6374897956848,
        "elapsed_time": 3439.9042615890503
      },
      "learning_rate": 0.0005,
      "batch_size": 32,
      "dataset_size": 10466,
      "total_parameters": 6632322,
      "model_display_name": "LSTM with Attention"
    },
    {
      "epoch": 4,
      "phase": "pretrain",
      "model_name": "lstmclassifier",
      "timestamp": 1763643723.2399104,
      "metrics": {
        "train_loss": 5.347708036986793,
        "perplexity": 210.12614418795528,
        "total_tokens": 5337660.0,
        "epoch_time": 1151.9069550037384,
        "elapsed_time": 4591.8316032886505
      },
      "learning_rate": 0.0005,
      "batch_size": 32,
      "dataset_size": 10466,
      "total_parameters": 6632322,
      "model_display_name": "LSTM with Attention"
    },
    {
      "epoch": 5,
      "phase": "pretrain",
      "model_name": "lstmclassifier",
      "timestamp": 1763644873.0584686,
      "metrics": {
        "train_loss": 1.8804665887501182,
        "perplexity": 6.556563367310244,
        "total_tokens": 5337660.0,
        "epoch_time": 1149.79643201828,
        "elapsed_time": 5741.650052547455
      },
      "learning_rate": 0.0005,
      "batch_size": 32,
      "dataset_size": 10466,
      "total_parameters": 6632322,
      "model_display_name": "LSTM with Attention"
    },
    {
      "epoch": 6,
      "phase": "pretrain",
      "model_name": "lstmclassifier",
      "timestamp": 1763646023.0480664,
      "metrics": {
        "train_loss": 0.8592443809640117,
        "perplexity": 2.3613757189659066,
        "total_tokens": 5337660.0,
        "epoch_time": 1149.9631206989288,
        "elapsed_time": 6891.639484882355
      },
      "learning_rate": 0.0005,
      "batch_size": 32,
      "dataset_size": 10466,
      "total_parameters": 6632322,
      "model_display_name": "LSTM with Attention"
    },
    {
      "epoch": 7,
      "phase": "pretrain",
      "model_name": "lstmclassifier",
      "timestamp": 1763647171.7807353,
      "metrics": {
        "train_loss": 0.6767120550318462,
        "perplexity": 1.9673983885728197,
        "total_tokens": 5337660.0,
        "epoch_time": 1148.6796431541443,
        "elapsed_time": 8040.372434139252
      },
      "learning_rate": 0.0005,
      "batch_size": 32,
      "dataset_size": 10466,
      "total_parameters": 6632322,
      "model_display_name": "LSTM with Attention"
    },
    {
      "epoch": 8,
      "phase": "pretrain",
      "model_name": "lstmclassifier",
      "timestamp": 1763648321.4607005,
      "metrics": {
        "train_loss": 0.6153653916425821,
        "perplexity": 1.8503325721112924,
        "total_tokens": 5337660.0,
        "epoch_time": 1149.636131286621,
        "elapsed_time": 9190.052206039429
      },
      "learning_rate": 0.0005,
      "batch_size": 32,
      "dataset_size": 10466,
      "total_parameters": 6632322,
      "model_display_name": "LSTM with Attention"
    }
  ],
  "finetune_history": [],
  "best_pretrain": {
    "epoch": 8,
    "loss": 0.6153653916425821,
    "perplexity": 1.8503325721112924
  }
}