{
  "model_name": "lstmclassifier",
  "total_pretrain_epochs": 8,
  "total_finetune_epochs": 0,
  "pretrain_history": [
    {
      "epoch": 1,
      "phase": "pretrain",
      "model_name": "lstmclassifier",
      "timestamp": 1763722110.6459455,
      "metrics": {
        "train_loss": 7.510952693660085,
        "perplexity": 1827.9541939027833,
        "total_tokens": 5337660.0,
        "epoch_time": 915.4528253078461,
        "elapsed_time": 915.4528954029083
      },
      "learning_rate": 0.0005,
      "batch_size": 32,
      "dataset_size": 10466,
      "total_parameters": 6830978,
      "model_display_name": "LSTM with Attention"
    },
    {
      "epoch": 2,
      "phase": "pretrain",
      "model_name": "lstmclassifier",
      "timestamp": 1763723019.1138783,
      "metrics": {
        "train_loss": 6.153551955048631,
        "perplexity": 470.3852101340753,
        "total_tokens": 5337660.0,
        "epoch_time": 908.4289162158966,
        "elapsed_time": 1823.9207434654236
      },
      "learning_rate": 0.0005,
      "batch_size": 32,
      "dataset_size": 10466,
      "total_parameters": 6830978,
      "model_display_name": "LSTM with Attention"
    },
    {
      "epoch": 3,
      "phase": "pretrain",
      "model_name": "lstmclassifier",
      "timestamp": 1763723927.1280546,
      "metrics": {
        "train_loss": 5.601593319962665,
        "perplexity": 270.857626663419,
        "total_tokens": 5337660.0,
        "epoch_time": 907.9883010387421,
        "elapsed_time": 2731.935128927231
      },
      "learning_rate": 0.0005,
      "batch_size": 32,
      "dataset_size": 10466,
      "total_parameters": 6830978,
      "model_display_name": "LSTM with Attention"
    },
    {
      "epoch": 4,
      "phase": "pretrain",
      "model_name": "lstmclassifier",
      "timestamp": 1763724835.2467868,
      "metrics": {
        "train_loss": 3.0438059677438036,
        "perplexity": 20.98495951909791,
        "total_tokens": 5337660.0,
        "epoch_time": 908.0901763439178,
        "elapsed_time": 3640.0536823272705
      },
      "learning_rate": 0.0005,
      "batch_size": 32,
      "dataset_size": 10466,
      "total_parameters": 6830978,
      "model_display_name": "LSTM with Attention"
    },
    {
      "epoch": 5,
      "phase": "pretrain",
      "model_name": "lstmclassifier",
      "timestamp": 1763725743.4802516,
      "metrics": {
        "train_loss": 0.696673156102983,
        "perplexity": 2.0070643982147294,
        "total_tokens": 5337660.0,
        "epoch_time": 908.2044448852539,
        "elapsed_time": 4548.287327289581
      },
      "learning_rate": 0.0005,
      "batch_size": 32,
      "dataset_size": 10466,
      "total_parameters": 6830978,
      "model_display_name": "LSTM with Attention"
    },
    {
      "epoch": 6,
      "phase": "pretrain",
      "model_name": "lstmclassifier",
      "timestamp": 1763726651.8753762,
      "metrics": {
        "train_loss": 0.29032064688096687,
        "perplexity": 1.3368560780404994,
        "total_tokens": 5337660.0,
        "epoch_time": 908.3698897361755,
        "elapsed_time": 5456.682472705841
      },
      "learning_rate": 0.0005,
      "batch_size": 32,
      "dataset_size": 10466,
      "total_parameters": 6830978,
      "model_display_name": "LSTM with Attention"
    },
    {
      "epoch": 7,
      "phase": "pretrain",
      "model_name": "lstmclassifier",
      "timestamp": 1763727566.2894776,
      "metrics": {
        "train_loss": 0.17483768743894448,
        "perplexity": 1.1910528780791527,
        "total_tokens": 5337660.0,
        "epoch_time": 914.385374546051,
        "elapsed_time": 6371.096319198608
      },
      "learning_rate": 0.0005,
      "batch_size": 32,
      "dataset_size": 10466,
      "total_parameters": 6830978,
      "model_display_name": "LSTM with Attention"
    },
    {
      "epoch": 8,
      "phase": "pretrain",
      "model_name": "lstmclassifier",
      "timestamp": 1763728474.399049,
      "metrics": {
        "train_loss": 0.12919344927933885,
        "perplexity": 1.1379102307894098,
        "total_tokens": 5337660.0,
        "epoch_time": 908.0798959732056,
        "elapsed_time": 7279.205981254578
      },
      "learning_rate": 0.0005,
      "batch_size": 32,
      "dataset_size": 10466,
      "total_parameters": 6830978,
      "model_display_name": "LSTM with Attention"
    }
  ],
  "finetune_history": [],
  "best_pretrain": {
    "epoch": 8,
    "loss": 0.12919344927933885,
    "perplexity": 1.1379102307894098
  }
}